import * as tf from "@tensorflow/tfjs";
import {ExperienceConfig, LearnConfig, OptimizerConfig} from "../config/types";
import {BatchTensorExperience} from "../game/experience/tensor";
import {Metrics} from "../model/worker/Metrics";
import {intToChoice} from "../psbot/handlers/battle/agent";

/**
 * Encapsulates the learning step of training, where the model is updated based
 * on experience generated by rollout games.
 */
export class Learn {
    /** Metrics logger. */
    private readonly metrics = Metrics.get(`${this.name}/learn`);
    /** Used for calculating gradients. */
    private readonly optimizer = Learn.getOptimizer(this.config.optimizer);
    /** Collection of trainable variables in the model. */
    private readonly variables = this.model.trainableWeights.map(
        w => w.read() as tf.Variable,
    );
    /** Used for logging inputs during loss calcs. */
    private readonly hookLayers: readonly tf.layers.Layer[] =
        this.model.layers.filter(l =>
            ["Dense", "SetAttention", "PoolingAttention"].includes(
                l.getClassName(),
            ),
        );

    /** Scale for TD target value. */
    private readonly tdScale = tf.scalar(
        this.expConfig.rewardDecay ** this.expConfig.steps,
        "float32",
    );

    /**
     * Creates a Learn object.
     *
     * @param name Name of the training run for logging.
     * @param model Model to train.
     * @param targetModel Model for computing TD targets. Can be set to the same
     * model to disable target model mechanism.
     * @param config Learning config.
     * @param expConfig Experience config for computing TD targets.
     */
    public constructor(
        public readonly name: string,
        private readonly model: tf.LayersModel,
        private readonly targetModel: tf.LayersModel,
        private readonly config: LearnConfig,
        private readonly expConfig: ExperienceConfig,
    ) {
        // Log initial weights.
        for (const weights of this.variables) {
            if (weights.size === 1) {
                const weightScalar = weights.asScalar();
                this.metrics?.scalar(
                    `${weights.name}/weights`,
                    weightScalar,
                    0,
                );
                tf.dispose(weightScalar);
            } else {
                this.metrics?.histogram(`${weights.name}/weights`, weights, 0);
            }
        }
    }

    /** Creates the neural network optimizer from config. */
    private static getOptimizer(config: OptimizerConfig): tf.Optimizer {
        switch (config.type) {
            case "sgd":
                return tf.train.sgd(config.learningRate);
            case "rmsprop":
                return tf.train.rmsprop(
                    config.learningRate,
                    config.decay,
                    config.momentum,
                );
            default: {
                const unsupported: never = config;
                throw new Error(
                    "Unsupported data type " +
                        `'${(unsupported as {type: string}).type}'`,
                );
            }
        }
    }

    /**
     * Performs a single batch update step.
     *
     * @param step Step number for logging.
     * @param batch Batch to train on.
     * @returns The loss for this batch.
     */
    public step(step: number, batch: BatchTensorExperience): tf.Scalar {
        return tf.tidy(() => {
            const preStep = process.hrtime.bigint();
            const storeBatchMetrics = step % this.config.metricsInterval === 0;

            const target = this.calculateTarget(
                batch.reward,
                batch.nextState,
                batch.choices,
                batch.done,
            );

            const hookedInputs: {[name: string]: tf.Tensor1D[]} = {};
            if (storeBatchMetrics) {
                for (const layer of this.hookLayers) {
                    layer.setCallHook(inputs =>
                        tf.tidy(() => {
                            if (!Array.isArray(inputs)) {
                                inputs = [inputs];
                            }
                            for (let i = 0; i < inputs.length; ++i) {
                                // Only take one example out of the batch to
                                // prevent excessive memory usage.
                                const input = tf.keep(
                                    inputs[i].slice(0, 1).flatten(),
                                );
                                let name = `${layer.name}/input`;
                                if (inputs.length > 1) {
                                    name += `/${i}`;
                                }
                                (hookedInputs[name] ??= []).push(input);
                            }
                        }),
                    );
                }
            }

            const {value: loss, grads} = this.optimizer.computeGradients(
                () => this.loss(batch.state, batch.action, target),
                this.variables,
            );
            this.optimizer.applyGradients(grads);

            const postStep = process.hrtime.bigint();
            const updateMs = Number(postStep - preStep) / 1e6;
            this.metrics?.scalar("update_ms", updateMs, step);
            this.metrics?.scalar(
                "update_throughput_s",
                this.config.batchSize /
                    (updateMs / 1e3) /*experiences per sec*/,
                step,
            );

            this.metrics?.scalar("loss", loss, step);

            if (storeBatchMetrics) {
                this.metrics?.histogram("target", target, step);
                target.dispose();

                for (const name in grads) {
                    if (Object.prototype.hasOwnProperty.call(grads, name)) {
                        const grad = grads[name];
                        if (grad.size === 1) {
                            this.metrics?.scalar(
                                `${name}/grads`,
                                grad.asScalar(),
                                step,
                            );
                        } else {
                            this.metrics?.histogram(
                                `${name}/grads`,
                                grad,
                                step,
                            );
                        }
                        grad.dispose();
                    }
                }

                for (const name in hookedInputs) {
                    if (
                        Object.prototype.hasOwnProperty.call(hookedInputs, name)
                    ) {
                        const inputs = hookedInputs[name];
                        const t = tf.concat1d(inputs);
                        this.metrics?.histogram(name, t, step);
                        t.dispose();
                        // Hooked inputs are tf.keep()'d so we have to dispose
                        // them manually.
                        tf.dispose(inputs);
                    }
                }

                for (const weights of this.variables) {
                    if (weights.size === 1) {
                        this.metrics?.scalar(
                            `${weights.name}/weights`,
                            weights.asScalar(),
                            step,
                        );
                    } else {
                        this.metrics?.histogram(
                            `${weights.name}/weights`,
                            weights,
                            step,
                        );
                    }
                }

                for (const layer of this.hookLayers) {
                    layer.clearCallHook();
                }

                // Since some of the histograms can use a lot of data, this
                // prevents buffer buildup over multiple learn steps.
                Metrics.flush();
            }

            return loss;
        });
    }

    /**
     * Calculates TD target for an experience batch.
     *
     * @param reward Reward tensor of shape `[batch]`.
     * @param nextState Tensors for next state, of shape `[batch, Ns...]`.
     * @param choices Choice legality mask for next state, of shape
     * `[batch, Nc]`.
     * @param done Terminal state indicator for next state, of shape `[batch]`.
     * @returns Temporal difference target, of shape `[batch]`.
     */
    private calculateTarget(
        reward: tf.Tensor,
        nextState: tf.Tensor[],
        choices: tf.Tensor,
        done: tf.Tensor,
    ): tf.Tensor {
        if (!Number.isFinite(this.expConfig.steps)) {
            return reward;
        }
        return tf.tidy(() => {
            let targetQ: tf.Tensor;
            if (!this.config.target) {
                // Vanilla DQN TD target: r + gamma * max_a(Q(s', a))
                let q = this.model.predictOnBatch(nextState) as tf.Tensor;
                // Also prevent illegal actions from influencing the value
                // function.
                q = tf.where(choices, q, -Infinity);
                targetQ = tf.max(q, -1);
            } else {
                targetQ = this.targetModel.predictOnBatch(
                    nextState,
                ) as tf.Tensor;
                if (this.config.target !== "double") {
                    // TD target with target net: r + gamma * max_a(Qt(s', a))
                    targetQ = tf.where(choices, targetQ, -Infinity);
                    targetQ = tf.max(targetQ, -1);
                } else {
                    // Double Q target: r + gamma * Qt(s', argmax_a(Q(s', a)))
                    let q = this.model.predictOnBatch(nextState) as tf.Tensor;
                    q = tf.where(choices, q, -Infinity);
                    const action = tf.argMax(q, -1);
                    const actionMask = tf.oneHot(action, intToChoice.length);
                    targetQ = tf.sum(tf.mul(targetQ, actionMask), -1);
                }
            }

            // Also mask out q values of terminal states.
            targetQ = tf.mul(targetQ, tf.sub(1, done));

            const target = tf.add(reward, tf.mul(this.tdScale, targetQ));
            return target;
        });
    }

    /**
     * Calculates training loss on an experience batch.
     *
     * @param state Tensors for state, of shape `[batch, Ns...]`.
     * @param action Action ids for each state, of shape `[batch]`.
     * @param target TD target of shape `[batch]`.
     */
    private loss(
        state: tf.Tensor[],
        action: tf.Tensor,
        target: tf.Tensor,
    ): tf.Scalar {
        return tf.tidy("loss", () => {
            let q = this.model.predictOnBatch(state) as tf.Tensor;
            const mask = tf.oneHot(action, intToChoice.length);
            q = tf.sum(tf.mul(q, mask), -1);
            return tf.losses.meanSquaredError(target, q);
        });
    }

    /** Cleans up dangling variables. */
    public cleanup(): void {
        this.tdScale.dispose();
        this.optimizer.dispose();
        Metrics.flush();
    }
}
